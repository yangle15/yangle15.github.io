
<!DOCTYPE HTML>
<!DOCTYPE html PUBLIC "" "">
<HTML lang="en-us">
<HEAD>
    <META content="IE=11.0000" http-equiv="X-UA-Compatible">
    <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Le Yang's Homepage</title>
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <style>
        body {
            font-family: 'Open Sans', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        .container {
            max-width: 1200px;
            margin: 40px auto;
            background-color: #ffffff;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            padding: 20px;
        }
        .menu-bar {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            background-color: #10406c; /* or any color you prefer */
            color: #ffffff;
            padding: 10px 20px;
            display: flex;
            justify-content: flex-end; /* Align menu items to the right */
            align-items: center;
            z-index: 1000; /* Ensure the menu bar is above other content */
        }
        .menu-bar a {
            color: #ffffff;
            text-decoration: none;
            margin-left: 15px;
            font-weight: 600;
        }
        .bio-section {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 20px;
        }
        .bio-content {
            width: calc(100% - 360px);
        }
        .profile-info {
            width: 300px;
            text-align: center;
        }
        .profile-info img {
            width: 100%;
            aspect-ratio: 16 / 10;
            object-fit: cover;
            margin-bottom: 10px;
            margin-top: 20px;
            border-radius: 16px;
        }
        .links a {
            display: inline-block;
            color: #636e72;
            font-size: 30px;
            margin: 1px 0px;
        }
        .papers-section {
            margin-top: 40px;
        }
        h2 {
            position: relative; /* Required for absolute positioning of the pseudo-element */
            padding-bottom: 10px; /* Adds some space between the title text and the line */
            color: #10406c;
            margin-bottom: 20px;
        }

        h2::after {
            content: ""; /* Required for pseudo-elements */
            position: absolute;
            left: 0;
            right: 0;
            bottom: 0; /* Aligns the line at the bottom of the element */
            height: 1px; /* Height of the line */
            background-color: #999999; /* Cool color for the line, matching your chosen h2 color */
        }
        .bio-section h3{
            color: #10406c;
        }
        /* .papers-section h2, .bio-content h2 {
            color: #0056b3;
            margin-bottom: 20px;
        } */
        .subsection-selector {
            cursor: pointer;
            background-color: #10406c;
            color: #ffffff;
            padding: 5px 10px;
            border-radius: 8px;
            margin-right: 10px;
            font-weight: bold;
            transition: background-color 0.3s;
        }
        .subsection-selector:hover, .subsection-selector.active {
            background-color: #10406c;
        }
        .subsection-content {
            display: none;
        }
        .subsection-content.active {
            display: block;
        }
        .paper-item {
            background-color: #e2e2e2;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
        }
        .paper-item img {
            width: 240px;
            height: auto;
            border-radius: 8px;
            object-fit: cover;
            margin-right: 20px;
        }
        .paper-details {
            flex-grow: 1;
        }
        .paper-details h3 {
            margin: 0;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
        }
        .paper-details h3 a {
            margin-left: 5px;
            font-size: 1.0rem;
            color: #10406c;
            font-weight: bold;
        }
        .paper-details p {
            margin: 10px 0;
            font-size: 14px;
        }
    </style>
</head>
<body>

<div class="menu-bar">
    <a href="#bio-section">Bio</a>
    <a href="#news-section">News</a>
    <a href="#papers-section">Selected Papers</a>
    <a href="#awards-section">Awards</a>
    <a href="#act-section">Activities</a>
    <a href="#contact-section">Contact</a>
</div>

<div class="container">
    <div class="bio-section" id="bio-section">
        <div class="bio-content">
            
            <h1><font color="0056b3">Le Yang (Êù®‰πê)</font></h1>
            <h1><font color="0056b3">Assistant Professor, Xi'an Jiaotong University</font></h1>
            <a href="https://scholar.google.com/citations?user=ju6v2acAAAAJ&hl=zh-CN"><b>Google Scholar</b></a>

            
            <h2>üßë‚Äçüéì Bio</h2>
            <p>I'm currently as an assistant professor at Xi'an Jiaotong University. My research focuses on deep learning and computer vision, in particular Dynamic Neural Networks, Efficient Learning/Inference of deep models, Video Understanding and Large Vision-Language Models(LVLMs).
            <br>
            
            üî•<b><font color="#0056b3">Our group are looking for self-motivated Master candidates and undergraduate student interns for the ongoing researches. Please drop me an email with your resume if you are interested.</font></b>

            <h3>üè´ Work Experience</h3>
            <DIV><UL>
                <Li>06/2021 - now,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Assistant Professor, &nbsp; Xi'an Jiaotong University.
                </Li>
                <Li>06/2021 - now,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Postdoctoral researcher, &nbsp; Xi'an Jiaotong University. advised by CAS Fellow, Prof. <a href="http://www.xjtu.edu.cn/info/1728/1960199.htm">Xiaohong Guan</a>.</li>
                </Li>
                <li>12/2021 - 07/2022, Visiting Scholar, ML Group, Aalto University.  &nbsp;&nbsp;  Advised by Prof. <a href="https://users.aalto.fi/~asolin/">Arno Solin</a>.</li>
            </UL></DIV>
            
            <h3>üìö Education</h3>
            <!-- <p>List your educational history and achievements.</p> -->
            <DIV><UL>
                <Li>09/2015 - 06/2021, Ph.D., Department of Automation, Tsinghua University. &nbsp;
                    Advised by Prof. <a href="https://www.au.tsinghua.edu.cn/info/1180/2108.htm">Shiji Song</a> and Prof. <a href="https://www.au.tsinghua.edu.cn/info/1075/3183.htm">Gao Huang</a>.
                <Li>09/2011 - 07/2015, B.E., Department of Automation, Northwestern Polytechnical University.  </Li>
            </UL></DIV>
            
            
            

        </div>
        <div class="profile-info">
            <img height="250" src="figures/mypic.jpg" alt="Your Photo">
            <div class="links">
                <a href="mailto:yangle15@xjtu.edu.cn"><i class="fas fa-envelope"></i></a>
                <a href="https://scholar.google.com/citations?user=ju6v2acAAAAJ&hl=en" target="_blank"><i class="fas fa-graduation-cap"></i></a>
                <a href="https://github.com/yangle15" target="_blank"><i class="fab fa-github"></i></a>
            
            </div>
        </div>
    </div>

    <div id="news-section">
        <h2>üì∞News</h2>
        <DIV><UL>
            <li>07/2024: üéâ Our works, <A href="https://arxiv.org/pdf/2407.12622">EfficientGEBD</A> is accepted at <b>ACM MM 2024</b>!</li>
            <li>07/2024: üéâ Three of our works (<A href="https://arxiv.org/abs/2407.03197">DyFADet</A>, DyBDet, and <A href="https://arxiv.org/abs/2402.19150">Typographic-attack-for-LVLM</A>) are accepted at <b>ECCV 2024</b>!</li>
            <li>06/2024: üèÜ Adadet is selected as <b><font color="red">the highly cited paper in ESI</font></b>!</li>
            <li>06/2023: üéâ Our work (<A href="https://ieeexplore.ieee.org/abstract/document/10155270">CoViFocus</A>) is accepted at <b><font color="BLACK">TCSVT</font></b>!</li>
            <li>05/2023: üéâ Our work (<A href="https://ieeexplore.ieee.org/abstract/document/10121781">AdaDet</A>) is accepted at <b><font color="BLACK">TCDS</font></b>!</li>

            <!-- <li>09/2022: Our work (Latency-aware spatial-wise dynamic networks) is accepted by <b>NeurIPS</b> 2022. </li>
            <li>07/2022: Our work (learning to weight samples of dynamic early-exiting networks) is accepted by <b>ECCV</b> 2022. </li> -->
        </UL></DIV>
    </div>

    <div class="papers-section" id="papers-section">
        <h2>üìÑ Publications</h2>
        <div>
            <span class="subsection-selector active" data-target="representative-publications" onclick="showSection('representative-publications')">Representative Publications</span>
            <span class="subsection-selector" data-target="Full publication list" onclick="showSection('Full publication list')">Full publication list</span>
        </div>

        <div id="Full publication list" class="subsection-content">
            <h1><font color="0056b3"> &nbsp;  </font></h1>
            (<a href="https://scholar.google.com/citations?user=ju6v2acAAAAJ&hl=en"><b>Full publication list on Google Scholar</b></a>)
            
        </div>

        <div id="representative-publications" class="subsection-content active">
            <!-- Insert representative publications here -->
            <div class="paper-item">
                <img src="figures/effgebd.jpg" alt="Paper Image">
                <div class="paper-details">
                    <h3>
                        Rethinking the Architecture Design for Efficient Generic Event Boundary Detection
                        <span class="links">
                            <a href="https://arxiv.org/pdf/2407.12622" target="_blank">[PDF]</a>
                            <a href="https://github.com/Ziwei-Zheng/EfficientGEBD/tree/main" target="_blank">[code]</a>
                        </span>
                    </h3>
                    <p>Ziwei Zheng, Zechuan Zhang, Yulin Wang, Shiji Song, Gao Huang, <b>Le Yang</b>*‚úâ.</p>
                    <p><I>ACM Multimedia (<b>ACM MM</b>), 2024</I></p>
                    <p>In this paper, we experimentally reexamine the architecture of GEBD models and uncover several surprising findings, demonstrating that some of sophisticated designs are unnecessary for building GEBD models. We also show that the GEBD models using image-domain backbones conducting the spatiotemporal learning in a spatial-then-temporal greedy manner can suffer from a distraction issue, which might be the inefficient villain for the GEBD. </p>
                </div>
            </div>
            
            <div class="paper-item">
                <img src="figures/dyfadet.png" alt="Paper Image">
                <div class="paper-details">
                    <h3>
                        DyFADet: Dynamic Feature Aggregation for Temporal Action Detection.
                        <span class="links">
                            <a href="https://arxiv.org/pdf/2407.03197" target="_blank">[PDF]</a>
                            <a href="https://github.com/yangle15/DyFADet-pytorch" target="_blank">[code]</a>
                        </span>
                    </h3>
                    <p><b>Le Yang</b>*‚úâ, Ziwei Zheng*, Yizeng Han, Hao Cheng, Shiji Song, Gao Huang, Fan Li.</p>
                    <p><I>European Conference on Computer Vision (<b>ECCV</b>), 2024</I></p>
                    <p>We propose a new dynamic feature aggregation module that can simultaneously adapt the kernel shape and parameters based on input. The TAD model based on DFA can boosts the performance by a large margin. </p>
                </div>
            </div>

            <div class="paper-item">
                <img src="figures/covifocus.png" alt="Paper Image">
                <div class="paper-details">
                    <h3>
                        Dynamic Spatial Focus for Efficient Compressed Video Action Recognition.
                        <span class="links">
                            <a href="https://ieeexplore.ieee.org/abstract/document/10155270" target="_blank">[PDF]</a>
                        </span>
                    </h3>
                    <p>Ziwei Zheng, <b>Le Yang</b>‚úâ, Yulin Wang, Miao Zhang, Lijun He, Gao Huang, Fan Li.</p>
                    <p><I>IEEE Transactions on Circuits and Systems for Video Technology (<b>T-CSVT</b>), 2023</I></p>
                    <p>We propose the fist dynamic spatial focus video recognition model for compressed video (such as MPEG4 and HEVC). </p>
                </div>
            </div>

            <div class="paper-item">
                <img src="figures/adadet.png" alt="Paper Image">
                <div class="paper-details">
                    <h3>
                        AdaDet: An Adaptive Object Detection System Based on Early-Exit Neural Networks.
                        <span class="links">
                            <a href="https://ieeexplore.ieee.org/abstract/document/10121781" target="_blank">[PDF]</a>
                        </span>
                    </h3>
                    <p><b>Le Yang</b>, Ziwei Zheng, Jian Wang, Shiji Song, Gao Huang, Fan Li‚úâ.</p>
                    <p><I>IEEE Transactions on Cognitive and Developmental Systems (<b>T-CDS</b>), 2023</I></p>
                    <p>We propose a novel early-exiting adaptive inference mechanism for object detection tasks. The images containing few-large-clear objects will exit from the network early during inference. Only these images containing multiple overlapping objects will be considered as hard samples and processed by the full network.</p>
                </div>
            </div>


            <div class="paper-item">
                    <img src="figures/survey.png" alt="Paper Image">
                    <div class="paper-details">
                        <h3>
                            Dynamic Neural Networks: A Survey
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2102.04906.pdf" target="_blank">[PDF]</a>
                                <a href="https://mp.weixin.qq.com/s/TG_HBAR7Jrec4X02sxNGEA" target="_blank">[Êô∫Ê∫êÁ§æÂå∫]</a>
                                <a href="https://jmq.h5.xeknow.com/s/2H6ZSj" target="_blank">[Êú∫Âô®‰πãÂøÉ-Âú®Á∫øËÆ≤Â∫ß]</a>
                                <a href="https://www.bilibili.com/video/BV19B4y1A7Wy?from=search&seid=12254026542403915477" target="_blank">[Bilibili]</a>
                                <a href="papers/Âä®ÊÄÅÁ•ûÁªèÁΩëÁªúÁ†îÁ©∂Ê¶ÇËø∞.pdf" target="_blank">[Slides]</a>
                            </span>
                        </h3>
                        <p>Yizeng Han*, Gao Huang*‚úâ, Shiji Song, <b>Le Yang</b>, Honghui Wang, Yulin Wang</p>
                        <p><I>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b><font color="black">T-PAMI</font></b>), 2021</I></p>
                        <p>In this survey, we comprehensively review the rapidly developing area, dynamic neural networks. The important research problems, e.g., architecture design, decision making scheme, and optimization technique, are reviewed systematically. We also discuss the open problems in this field together with interesting future research directions.</p>
                    </div>
            </div>     
            

            <div class="paper-item">
                <img src="figures/cdnetv2.gif" alt="Paper Image">
                <div class="paper-details">
                    <h3>
                        CondenseNet V2: Sparse Feature Reactivation for Deep Networks.
                        <span class="links">
                            <A href="https://arxiv.org/abs/2104.04382"    target="_blank">[PDF]</A><A href="https://github.com/jianghaojun/CondenseNetV2"    target="_blank">[code]</A><A href="https://zhuanlan.zhihu.com/p/364846540"    target="_blank">[Áü•‰πé]</A>
                        </span>
                    </h3>
                    <p><b>Le Yang</b>*, Haojun Jiang*, Ruojin Cai, Yulin Wang, Shiji Song, Gao Huang‚úâ, Qi Tian.</p>
                    <p><I>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021</I></p>
                    <p>We propose a new feature reusing method in deep networks through dense connectivity, which can simultaneously learn to 1) selectively reuse a set of most important features from preceding layers; and 2) actively update a set of preceding features to increase their utility for later layers. </p>
                </div>
            </div>

            <div class="paper-item">
                <img src="figures/RANet.gif" alt="Paper Image">
                <div class="paper-details">
                    <h3>
                        Resolution Adaptive Networks for Efficient Inference.
                        <span class="links">
                            <A href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Resolution_Adaptive_Networks_for_Efficient_Inference_CVPR_2020_paper.pdf"    target="_blank">[PDF]</A>
                            <A href="https://github.com/yangle15/RANet-pytorch"    target="_blank">[code]</A>   
                        </span>
                    </h3>
                    <p><b>Le Yang</b>*, Yizeng Han*, Xi Chen*, Shiji Song, Jifeng Dai, Gao Huang‚úâ
                    <p><I>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2020</I></p>
                    <p>The proposed Resolution Adaptive Network (RANet) makes use of spatial redundancy in images to conduct the adaptive inference for the first time. The RANet is inspired by the intuition that low-resolution representations are sufficient for classifying ‚Äúeasy‚Äù inputs containing large objects with prototypical features, while only some ‚Äúhard‚Äù samples need spatially detailed information. </p>
                </div>
            </div>
        
        </div>

        

        <H2 id="awards-section">üéñ Awards</H2>    
            <DIV><UL>
                <li><b>The <a href="https://zhuanlan.zhihu.com/p/373281444">2021 Postdoctoral Innovative Talent Program</a>(2021ÂçöÊñ∞ËÆ°Âàí), advised by CAS Fellow, Prof. <a href="http://www.xjtu.edu.cn/info/1728/1960199.htm">Xiaohong Guan</a>, 2021.</b></li> 
                <li><b>Outstanding Graduate of Tsinghua University (Ê∏ÖÂçéÂ§ßÂ≠¶‰ºòÁßÄÊØï‰∏öÁîü)</b>, 2021.</li>
                <li><b>Outstanding Graduate of Beijing (Âåó‰∫¨Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü)</b>, 2021.</li>
                

        </UL></DIV>

        <H2 id="act-section">üßë‚Äçüíª Professional Activities</H2>
            <DIV><UL>
                <li>Technical Programm Committee of <a href="https://ubicomp-cpd.com/2023.html">UbiComp2023-CPD</a>, <a href="https://picasso-2024.github.io/mobicom-picasso-2024.html">MobiCom2024-PICASSO</a>.</li>
  
                <li>Program Committee (PC) member of IJCAI 2021.</li>
                
                <li>Reviewer for IJCV, T-PAMI, T-NNLS, T-Cyber, T-CSVT, ...</li>        
                
                <li>Reviewer for CVPR, ICCV, IJCAI, NeurIPS, ICML, ICLR...</li>      

            </UL></DIV>
    

        <H2 id="contact-section">üìß Contact</H2>
            <DIV><UL>
                <LI> yangle15 at xjtu dot edu dot cn</LI>
                <LI> Address: Room 140, Pengkang building, Xingqing Campus, Xi'an Jiaotong University, Xi'an 710049, China.</LI>
            </UL></DIV>
    </div>
</div>


<script>
    function showSection(sectionId) {
        document.querySelectorAll('.subsection-content').forEach(function(content) {
            content.classList.remove('active');
        });
        document.querySelectorAll('.subsection-selector').forEach(function(selector) {
            selector.classList.remove('active');
        });
        document.getElementById(sectionId).classList.add('active');
        document.querySelector('[data-target="' + sectionId + '"]').classList.add('active');
    }
</script>

</body>
</html>
